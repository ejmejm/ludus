{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Breakout Example\n",
    "\n",
    "### Atari Breakout\n",
    "\n",
    "Please do note that this example may take a long time to train.\n",
    "\n",
    "With the default 4 threads runnning on an 8-core CPU with a GTX 1080 Ti, it will take several hours to train to a decent level of play.\n",
    "\n",
    "Running on a platform with more GPU power and a larger cluster of CPUs could siginificantly reduce training time.\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1705.05363.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.backend import categorical_crossentropy\n",
    "from ludus.policies import BaseTrainer\n",
    "from ludus.env import EnvController\n",
    "from ludus.utils import preprocess_atari, reshape_train_var\n",
    "from ludus.memory import MTMemoryBuffer\n",
    "import gym\n",
    "# Super Mario stuff\n",
    "from nes_py.wrappers import BinarySpaceToDiscreteSpaceEnv\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "    env = BinarySpaceToDiscreteSpaceEnv(env, SIMPLE_MOVEMENT)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMTrainer(BaseTrainer):\n",
    "    def __init__(self, in_op, out_op, value_out_op, act_type='discrete', sess=None, clip_val=0.2, ppo_iters=80,\n",
    "                 target_kl=0.01, v_coef=1., entropy_coef=0.01):\n",
    "        self.value_out_op = value_out_op\n",
    "        self.clip_val = clip_val\n",
    "        self.ppo_iters = ppo_iters\n",
    "        self.target_kl = target_kl\n",
    "        self.v_coef = v_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        # ICM parameters, TODO: make parameters for these\n",
    "        self.ro_coef = 0.5\n",
    "        self.beta = 0.2\n",
    "        self.eta = 1\n",
    "        self.r_i_coef = 1\n",
    "        self.r_e_coef = 0.2\n",
    "        \n",
    "        super().__init__(in_op, out_op, act_type, sess)\n",
    "        \n",
    "    def _create_ICM(self, optimizer=tf.train.AdamOptimizer()):\n",
    "        feature_dim = 128 # TODO: make a parameter for this\n",
    "        \n",
    "        # Create placeholder\n",
    "        self.next_obs_holders = tf.placeholder(tf.float32, shape=self.in_op.shape)\n",
    "        \n",
    "        # Observation feature encoder\n",
    "        with tf.variable_scope('feature_encoder'):\n",
    "            enc_layers = [\n",
    "                Conv2D(32, 3, activation=tf.nn.tanh, name='fe_conv'),\n",
    "                MaxPool2D(2, name='fe_max_pool'),\n",
    "                Conv2D(32, 3, activation=tf.nn.tanh, name='fe_conv2'),\n",
    "                MaxPool2D(2, name='fe_max_pool2'),\n",
    "                Conv2D(32, 3, activation=tf.nn.tanh, name='fe_conv3'),\n",
    "                MaxPool2D(2, name='fe_max_pool3'),\n",
    "                Flatten(name='fe_flattened'),\n",
    "                Dense(feature_dim, activation=tf.nn.tanh, use_bias=False, name='fe_dense')\n",
    "            ]\n",
    "            \n",
    "            # Encoding state\n",
    "            self.f_obs = enc_layers[0](self.in_op)\n",
    "            for i in range(1, len(enc_layers)):\n",
    "                self.f_obs = enc_layers[i](self.f_obs)\n",
    "            \n",
    "            # Encoding the next state\n",
    "            self.f_obs_next = enc_layers[0](self.next_obs_holders)\n",
    "            for i in range(1, len(enc_layers)):\n",
    "                self.f_obs_next = enc_layers[i](self.f_obs_next)\n",
    "            \n",
    "        # State predictor forward model\n",
    "        with tf.variable_scope('forward_model'):\n",
    "            self.state_act_pair = tf.concat([self.out_op, self.f_obs], axis=1)\n",
    "            self.sp_dense = Dense(64, activation=tf.nn.tanh)(self.state_act_pair)\n",
    "            self.f_obs_next_pred = Dense(feature_dim, activation=tf.nn.tanh, use_bias=False)(self.sp_dense)\n",
    "        \n",
    "        # Inverse dynamics model (predicting action)\n",
    "        with tf.variable_scope('inverse_model'):\n",
    "            self.state_state_pair = tf.concat([self.f_obs, self.f_obs_next], axis=1)\n",
    "            self.act_preds = Dense(64, activation=tf.nn.relu)(self.state_state_pair)\n",
    "            # TODO: softmax only works for discrete\n",
    "            self.act_preds = Dense(self.out_op.shape[1], use_bias=False, activation=tf.nn.softmax)(self.act_preds)\n",
    "        \n",
    "        # Calculating intrinsic reward\n",
    "        self.obs_pred_diff = self.f_obs_next_pred - self.f_obs_next\n",
    "        self.r_i = 0.5 * self.eta * tf.reduce_sum(self.obs_pred_diff ** 2, axis=1) # Fix these squares (Probably okay)\n",
    "        self.r_ie = self.r_i_coef * self.r_i # + self.r_e_coef * self.reward_holders\n",
    "        \n",
    "        # Calculating losses\n",
    "        self.pre_loss_i = categorical_crossentropy(self.act_masks, self.act_preds) # tf.reduce_sum((self.act_holders - self.act_pred) ** 2, axis=1)\n",
    "        self.pre_loss_f = 0.5 * tf.reduce_sum(self.obs_pred_diff ** 2, axis=1)\n",
    "        \n",
    "        self.loss_i = (1 - self.beta) * tf.reduce_mean(self.pre_loss_i)\n",
    "        self.loss_f = self.beta * tf.reduce_mean(self.pre_loss_f)\n",
    "        self.loss_p = -self.ro_coef * tf.reduce_mean(self.r_ie)\n",
    "        \n",
    "        # Making update functions\n",
    "        self.i_train_vars = tf.trainable_variables(scope='feature_encoder') + tf.trainable_variables(scope='inverse_model')\n",
    "        self.f_train_vars = tf.trainable_variables(scope='forward_model')\n",
    "        self.p_train_vars = [var for var in tf.trainable_variables() if var not in (self.i_train_vars + self.f_train_vars)]\n",
    "        \n",
    "        self.li_update = optimizer.minimize(self.loss_i, var_list=self.i_train_vars)\n",
    "        self.lf_update = optimizer.minimize(self.loss_f, var_list=self.f_train_vars)\n",
    "        self.lp_update = optimizer.minimize(self.loss_p, var_list=self.p_train_vars)\n",
    "        \n",
    "        self.icm_updates = [self.li_update, self.lf_update, self.lp_update]\n",
    "        self.losses = [self.loss_i, self.loss_f, self.loss_p]\n",
    "        \n",
    "    def _create_discrete_trainer(self, optimizer=tf.train.AdamOptimizer()):\n",
    "        \"\"\"\n",
    "        Creates a function for vanilla policy training with a discrete action space\n",
    "        \"\"\"\n",
    "        # First passthrough\n",
    "        \n",
    "        self.act_holders = tf.placeholder(tf.int32, shape=[None])\n",
    "        self.reward_holders = tf.placeholder(tf.float32, shape=[None])\n",
    "        \n",
    "        self.act_masks = tf.one_hot(self.act_holders, self.out_op.shape[1].value, dtype=tf.float32)\n",
    "        self.resp_acts = tf.reduce_sum(self.act_masks *  self.out_op, axis=1)\n",
    "        \n",
    "        self.advantages = self.reward_holders - tf.squeeze(self.value_out_op)\n",
    "        \n",
    "        self._create_ICM()\n",
    "        \n",
    "        # Second passthrough\n",
    "        \n",
    "        self.advatange_holders = tf.placeholder(dtype=tf.float32, shape=self.advantages.shape)\n",
    "        self.old_prob_holders = tf.placeholder(dtype=tf.float32, shape=self.resp_acts.shape)\n",
    " \n",
    "        self.policy_ratio = self.resp_acts / self.old_prob_holders\n",
    "        self.clipped_ratio = tf.clip_by_value(self.policy_ratio, 1 - self.clip_val, 1 + self.clip_val)\n",
    "\n",
    "        self.min_loss = tf.minimum(self.policy_ratio * self.advatange_holders, self.clipped_ratio * self.advatange_holders)\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "        # Actor update\n",
    "        \n",
    "        self.kl_divergence = tf.reduce_mean(tf.log(self.old_prob_holders) - tf.log(self.resp_acts))\n",
    "        self.actor_loss = -tf.reduce_mean(self.min_loss)\n",
    "        self.actor_update = self.optimizer.minimize(self.actor_loss)\n",
    "\n",
    "        # Value update\n",
    "        \n",
    "        self.value_loss = tf.reduce_mean(tf.square(self.reward_holders - tf.squeeze(self.value_out_op)))\n",
    "        self.value_update = self.optimizer.minimize(self.value_loss)\n",
    "        \n",
    "        # Combined update\n",
    "        \n",
    "        self.entropy = -tf.reduce_mean(tf.reduce_sum(self.out_op * tf.log(1. / tf.clip_by_value(self.out_op, 1e-8, 1.0)), axis=1))\n",
    "        self.combined_loss = self.actor_loss + self.v_coef * self.value_loss + self.entropy_coef * self.entropy\n",
    "        self.combined_update = self.optimizer.minimize(self.combined_loss)\n",
    "        \n",
    "        def update_func(train_data, train_type=0):\n",
    "            if train_type == 0:\n",
    "                i_rew, li, lf, lp, _, _, _ = self.sess.run([tf.reduce_mean(self.r_i)] + self.losses + self.icm_updates, \n",
    "                                       feed_dict={self.in_op: reshape_train_var(train_data[:, 0]),\n",
    "                                                  self.act_holders: reshape_train_var(train_data[:, 1]),\n",
    "                                                  self.reward_holders: train_data[:, 2],\n",
    "                                                  self.next_obs_holders: reshape_train_var(train_data[:, 3])})\n",
    "                return i_rew, [li, lf, lp]\n",
    "            else:\n",
    "                self.old_probs, self.old_advantages = self.sess.run([self.resp_acts, self.advantages], \n",
    "                                        feed_dict={self.in_op: reshape_train_var(train_data[:, 0]),\n",
    "                                                   self.act_holders: train_data[:, 1],\n",
    "                                                   self.reward_holders: train_data[:, 2]})\n",
    "\n",
    "                for i in range(self.ppo_iters):\n",
    "                    kl_div, _ = self.sess.run([self.kl_divergence, self.combined_update], \n",
    "                                   feed_dict={self.in_op: reshape_train_var(train_data[:, 0]),\n",
    "                                        self.act_holders: reshape_train_var(train_data[:, 1]),\n",
    "                                        self.reward_holders: train_data[:, 2],\n",
    "                                        self.old_prob_holders: self.old_probs,\n",
    "                                        self.advatange_holders: self.old_advantages})\n",
    "                    if kl_div > 1.5 * self.target_kl:\n",
    "                        break\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        return update_func\n",
    "        \n",
    "    def _create_continuous_trainer(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ejmejm/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ejmejm/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ejmejm/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "env = make_env() # This instance of the environment is only used\n",
    "                              # to get action dimensions\n",
    "in_shape = [42, 42, 4] # Size of reshaped observations\n",
    "\n",
    "# Creating a conv net for the policy and value estimator\n",
    "obs_op = Input(shape=in_shape)\n",
    "conv1 = Conv2D(32, 3, activation='relu')(obs_op)\n",
    "max_pool1 = MaxPool2D(2, 2)(conv1)\n",
    "conv2 = Conv2D(32, 3, activation='relu')(max_pool1)\n",
    "max_pool2 = MaxPool2D(2, 2)(conv2)\n",
    "conv3 = Conv2D(32, 3, activation='relu')(max_pool2)\n",
    "max_pool3 = MaxPool2D(2, 2)(conv3)\n",
    "flattened = Flatten()(max_pool3)\n",
    "dense1 = Dense(128, activation='relu')(flattened)\n",
    "dense2 = Dense(256, activation='relu')(dense1)\n",
    "dense3 = Dense(128, activation='relu')(dense1)\n",
    "\n",
    "# Output probability distribution over possible actions\n",
    "act_probs_op = Dense(env.action_space.n, activation='softmax')(dense2)\n",
    "\n",
    "# Output value of observed state\n",
    "value_op = Dense(1)(dense3)\n",
    "\n",
    "# Wrap a Proximal Policy Optimization Trainer on top of the network\n",
    "network = IMTrainer(obs_op, act_probs_op, value_op, act_type='discrete', ppo_iters=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10000 # Total episodes of data to collect\n",
    "max_steps = 1024 # Max number of frames per game\n",
    "batch_size = 4 # Smaller = faster, larger = stabler\n",
    "print_freq = 1 # How many training updates between printing progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_hist = {} # Keeps track of up to 3 previous frames for each agent\n",
    "\n",
    "# Create observation transformation that adds the two last frames on\n",
    "# as two extra dimensions\n",
    "def new_obs_transform(obs, agent_id):\n",
    "    new_frame = preprocess_atari(obs.squeeze(), size=(42, 42)) # First preprocess the new frame\n",
    "    \n",
    "    if agent_id in agent_hist: # Case for a continued episode\n",
    "        agent_hist[agent_id] = agent_hist[agent_id][1:]\n",
    "        agent_hist[agent_id].append(new_frame)\n",
    "    else: # Case for a new episode\n",
    "        agent_hist[agent_id] = [new_frame, new_frame, new_frame, new_frame]\n",
    "    \n",
    "    # Format the data\n",
    "    arr = np.array(agent_hist[agent_id])\n",
    "    return np.swapaxes(arr, 0, 3).squeeze()\n",
    "\n",
    "############################################################\n",
    "############################################################\n",
    "\n",
    "mtmb = MTMemoryBuffer() # Create a memory buffer to store the episode data\n",
    "\n",
    "# Edit the memory buffer's start_rollout function so that every time\n",
    "# an episode ends, it resets the respective agent's history\n",
    "old_start_rollout = mtmb.start_rollout\n",
    "\n",
    "def new_start_rollout(agent_id):\n",
    "    old_start_rollout(agent_id)\n",
    "    agent_hist.pop(agent_id, None)\n",
    "    \n",
    "mtmb.start_rollout = new_start_rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment controller for generating game data\n",
    "ec = EnvController(make_env, n_threads=4, memory_buffer=mtmb)\n",
    "# Set the preprocessing function for observations\n",
    "ec.set_obs_transform(new_obs_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update #1, Avg Reward (E, I): 477.25, 4.468448638916016\n",
      "I: 1.576297402381897, F: 0.893689751625061, P: -2.234224319458008\n",
      "\n",
      "Update #2, Avg Reward (E, I): 503.0, 4.810794830322266\n",
      "I: 1.5645735263824463, F: 0.9621589779853821, P: -2.405397415161133\n",
      "\n",
      "Update #3, Avg Reward (E, I): 563.0, 4.931573867797852\n",
      "I: 1.5636327266693115, F: 0.9863147735595703, P: -2.465786933898926\n",
      "\n",
      "Update #4, Avg Reward (E, I): 500.0, 4.566880226135254\n",
      "I: 1.5561046600341797, F: 0.9133760333061218, P: -2.283440113067627\n",
      "\n",
      "Update #5, Avg Reward (E, I): 565.75, 4.51606559753418\n",
      "I: 1.555545687675476, F: 0.9032131433486938, P: -2.25803279876709\n",
      "\n",
      "Update #6, Avg Reward (E, I): 530.5, 4.299077033996582\n",
      "I: 1.545089602470398, F: 0.8598154187202454, P: -2.149538516998291\n",
      "\n",
      "Update #7, Avg Reward (E, I): 562.75, 4.18452262878418\n",
      "I: 1.530725359916687, F: 0.8369045257568359, P: -2.09226131439209\n",
      "\n",
      "Update #8, Avg Reward (E, I): 566.75, 4.242562770843506\n",
      "I: 1.5090140104293823, F: 0.848512589931488, P: -2.121281385421753\n",
      "\n",
      "Update #9, Avg Reward (E, I): 533.5, 4.536107540130615\n",
      "I: 1.5009958744049072, F: 0.9072214961051941, P: -2.2680537700653076\n",
      "\n",
      "Update #10, Avg Reward (E, I): 498.5, 4.6183552742004395\n",
      "I: 1.4742414951324463, F: 0.9236710667610168, P: -2.3091776371002197\n",
      "\n",
      "Update #11, Avg Reward (E, I): 657.5, 4.559241771697998\n",
      "I: 1.4968359470367432, F: 0.9118483662605286, P: -2.279620885848999\n",
      "\n",
      "Update #12, Avg Reward (E, I): 628.0, 3.9767708778381348\n",
      "I: 1.5218592882156372, F: 0.7953541874885559, P: -1.9883854389190674\n",
      "\n",
      "Update #13, Avg Reward (E, I): 721.5, 3.200380802154541\n",
      "I: 1.5170859098434448, F: 0.6400761604309082, P: -1.6001904010772705\n",
      "\n",
      "Update #14, Avg Reward (E, I): 500.0, 2.7431743144989014\n",
      "I: 1.5343536138534546, F: 0.5486348867416382, P: -1.3715871572494507\n",
      "\n",
      "Update #15, Avg Reward (E, I): 531.5, 2.5180583000183105\n",
      "I: 1.5387191772460938, F: 0.50361168384552, P: -1.2590291500091553\n",
      "\n",
      "Update #16, Avg Reward (E, I): 501.0, 2.448568344116211\n",
      "I: 1.5408636331558228, F: 0.4897136688232422, P: -1.2242841720581055\n",
      "\n",
      "Update #17, Avg Reward (E, I): 561.25, 2.410994529724121\n",
      "I: 1.548388123512268, F: 0.48219892382621765, P: -1.2054972648620605\n",
      "\n",
      "Update #18, Avg Reward (E, I): 644.75, 2.264436721801758\n",
      "I: 1.5452452898025513, F: 0.4528873562812805, P: -1.132218360900879\n",
      "\n",
      "Update #19, Avg Reward (E, I): 561.5, 1.913077473640442\n",
      "I: 1.5421310663223267, F: 0.38261550664901733, P: -0.956538736820221\n",
      "\n",
      "Update #20, Avg Reward (E, I): 601.25, 1.7153995037078857\n",
      "I: 1.5347537994384766, F: 0.34307989478111267, P: -0.8576997518539429\n",
      "\n",
      "Update #21, Avg Reward (E, I): 616.5, 1.5437350273132324\n",
      "I: 1.5384559631347656, F: 0.3087470233440399, P: -0.7718675136566162\n",
      "\n",
      "Update #22, Avg Reward (E, I): 562.25, 1.2580090761184692\n",
      "I: 1.5324474573135376, F: 0.25160181522369385, P: -0.6290045380592346\n",
      "\n",
      "Update #23, Avg Reward (E, I): 501.75, 1.0882409811019897\n",
      "I: 1.5296812057495117, F: 0.2176481932401657, P: -0.5441204905509949\n",
      "\n",
      "Update #24, Avg Reward (E, I): 622.5, 1.0742712020874023\n",
      "I: 1.5259697437286377, F: 0.21485424041748047, P: -0.5371356010437012\n",
      "\n",
      "Update #25, Avg Reward (E, I): 563.0, 1.1573610305786133\n",
      "I: 1.5120209455490112, F: 0.2314722090959549, P: -0.5786805152893066\n",
      "\n",
      "Update #26, Avg Reward (E, I): 622.5, 1.2559123039245605\n",
      "I: 1.4992687702178955, F: 0.2511824667453766, P: -0.6279561519622803\n",
      "\n",
      "Update #27, Avg Reward (E, I): 521.5, 1.4142405986785889\n",
      "I: 1.4797698259353638, F: 0.2828481197357178, P: -0.7071202993392944\n",
      "\n",
      "Update #28, Avg Reward (E, I): 596.75, 1.665521502494812\n",
      "I: 1.442309021949768, F: 0.33310431241989136, P: -0.832760751247406\n",
      "\n",
      "Update #29, Avg Reward (E, I): 686.0, 2.273716926574707\n",
      "I: 1.3794571161270142, F: 0.4547433853149414, P: -1.1368584632873535\n",
      "\n",
      "Update #30, Avg Reward (E, I): 590.75, 3.2658746242523193\n",
      "I: 1.3110449314117432, F: 0.6531749367713928, P: -1.6329373121261597\n",
      "\n",
      "Update #31, Avg Reward (E, I): 611.75, 4.03990364074707\n",
      "I: 1.3501667976379395, F: 0.8079807162284851, P: -2.019951820373535\n",
      "\n",
      "Update #32, Avg Reward (E, I): 471.75, 3.1788763999938965\n",
      "I: 1.4064931869506836, F: 0.6357752680778503, P: -1.5894381999969482\n",
      "\n",
      "Update #33, Avg Reward (E, I): 785.75, 2.1583194732666016\n",
      "I: 1.4413493871688843, F: 0.4316639006137848, P: -1.0791597366333008\n",
      "\n",
      "Update #34, Avg Reward (E, I): 685.25, 1.7504699230194092\n",
      "I: 1.5014574527740479, F: 0.3500939905643463, P: -0.8752349615097046\n",
      "\n",
      "Update #35, Avg Reward (E, I): 562.75, 1.5562517642974854\n",
      "I: 1.5278892517089844, F: 0.31125035881996155, P: -0.7781258821487427\n",
      "\n",
      "Update #36, Avg Reward (E, I): 735.5, 1.38772451877594\n",
      "I: 1.5371513366699219, F: 0.27754491567611694, P: -0.69386225938797\n",
      "\n",
      "Update #37, Avg Reward (E, I): 622.5, 1.3051838874816895\n",
      "I: 1.5446051359176636, F: 0.26103678345680237, P: -0.6525919437408447\n",
      "\n",
      "Update #38, Avg Reward (E, I): 500.0, 1.3754751682281494\n",
      "I: 1.5455644130706787, F: 0.27509504556655884, P: -0.6877375841140747\n",
      "\n",
      "Update #39, Avg Reward (E, I): 616.25, 1.5369997024536133\n",
      "I: 1.5499842166900635, F: 0.3073999583721161, P: -0.7684998512268066\n",
      "\n",
      "Update #40, Avg Reward (E, I): 562.0, 1.4500129222869873\n",
      "I: 1.5500580072402954, F: 0.29000258445739746, P: -0.7250064611434937\n",
      "\n",
      "Update #41, Avg Reward (E, I): 533.5, 1.34170663356781\n",
      "I: 1.5497981309890747, F: 0.2683413326740265, P: -0.670853316783905\n",
      "\n",
      "Update #42, Avg Reward (E, I): 503.0, 1.2711772918701172\n",
      "I: 1.5488108396530151, F: 0.25423547625541687, P: -0.6355886459350586\n",
      "\n",
      "Update #43, Avg Reward (E, I): 567.0, 1.180397868156433\n",
      "I: 1.5446189641952515, F: 0.23607957363128662, P: -0.5901989340782166\n",
      "\n",
      "Update #44, Avg Reward (E, I): 500.0, 1.0912985801696777\n",
      "I: 1.5420039892196655, F: 0.21825972199440002, P: -0.5456492900848389\n",
      "\n",
      "Update #45, Avg Reward (E, I): 566.0, 1.0920798778533936\n",
      "I: 1.5451022386550903, F: 0.2184159755706787, P: -0.5460399389266968\n",
      "\n",
      "Update #46, Avg Reward (E, I): 562.75, 1.0432313680648804\n",
      "I: 1.5437554121017456, F: 0.2086462825536728, P: -0.5216156840324402\n",
      "\n",
      "Update #47, Avg Reward (E, I): 846.5, 1.03830885887146\n",
      "I: 1.5475257635116577, F: 0.20766177773475647, P: -0.51915442943573\n",
      "\n",
      "Update #48, Avg Reward (E, I): 500.0, 1.0384061336517334\n",
      "I: 1.5545402765274048, F: 0.20768122375011444, P: -0.5192030668258667\n",
      "\n",
      "Update #49, Avg Reward (E, I): 530.5, 1.0304210186004639\n",
      "I: 1.5512336492538452, F: 0.206084206700325, P: -0.5152105093002319\n",
      "\n",
      "Update #50, Avg Reward (E, I): 533.5, 1.1264169216156006\n",
      "I: 1.552477240562439, F: 0.22528338432312012, P: -0.5632084608078003\n",
      "\n",
      "Update #51, Avg Reward (E, I): 624.5, 1.2187296152114868\n",
      "I: 1.5538028478622437, F: 0.24374592304229736, P: -0.6093648076057434\n",
      "\n",
      "Update #52, Avg Reward (E, I): 749.5, 1.321141242980957\n",
      "I: 1.5533835887908936, F: 0.2642282545566559, P: -0.6605706214904785\n",
      "\n",
      "Update #53, Avg Reward (E, I): 498.25, 1.3573012351989746\n",
      "I: 1.5528109073638916, F: 0.27146026492118835, P: -0.6786506175994873\n",
      "\n",
      "Update #54, Avg Reward (E, I): 533.5, 1.38981032371521\n",
      "I: 1.5552406311035156, F: 0.2779620587825775, P: -0.694905161857605\n",
      "\n",
      "Update #55, Avg Reward (E, I): 562.5, 1.447845220565796\n",
      "I: 1.5537538528442383, F: 0.28956905007362366, P: -0.723922610282898\n",
      "\n",
      "Update #56, Avg Reward (E, I): 599.5, 1.410158395767212\n",
      "I: 1.5534340143203735, F: 0.28203168511390686, P: -0.705079197883606\n",
      "\n",
      "Update #57, Avg Reward (E, I): 562.75, 1.355245590209961\n",
      "I: 1.5517630577087402, F: 0.2710491120815277, P: -0.6776227951049805\n",
      "\n",
      "Update #58, Avg Reward (E, I): 560.5, 1.2493360042572021\n",
      "I: 1.548417329788208, F: 0.24986720085144043, P: -0.6246680021286011\n",
      "\n",
      "Update #59, Avg Reward (E, I): 556.5, 1.183510661125183\n",
      "I: 1.5475553274154663, F: 0.23670212924480438, P: -0.5917553305625916\n",
      "\n",
      "Update #60, Avg Reward (E, I): 532.0, 1.200201392173767\n",
      "I: 1.5411134958267212, F: 0.24004028737545013, P: -0.6001006960868835\n",
      "\n",
      "Update #61, Avg Reward (E, I): 689.0, 1.3304754495620728\n",
      "I: 1.5277671813964844, F: 0.2660951018333435, P: -0.6652377247810364\n",
      "\n",
      "Update #62, Avg Reward (E, I): 558.25, 1.7070614099502563\n",
      "I: 1.5034383535385132, F: 0.3414122760295868, P: -0.8535307049751282\n",
      "\n",
      "Update #63, Avg Reward (E, I): 533.5, 2.4005215167999268\n",
      "I: 1.4723453521728516, F: 0.4801042973995209, P: -1.2002607583999634\n",
      "\n",
      "Update #64, Avg Reward (E, I): 533.5, 3.595947742462158\n",
      "I: 1.4327620267868042, F: 0.7191895842552185, P: -1.797973871231079\n",
      "\n",
      "Update #65, Avg Reward (E, I): 503.0, 5.2867021560668945\n",
      "I: 1.3806915283203125, F: 1.0573405027389526, P: -2.6433510780334473\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update #66, Avg Reward (E, I): 565.25, 7.85507869720459\n",
      "I: 1.2684125900268555, F: 1.57101571559906, P: -3.927539348602295\n",
      "\n",
      "Update #67, Avg Reward (E, I): 564.0, 11.23826789855957\n",
      "I: 0.9952138066291809, F: 2.2476537227630615, P: -5.619133949279785\n",
      "\n",
      "Update #68, Avg Reward (E, I): 544.75, 14.776544570922852\n",
      "I: 0.5438024401664734, F: 2.9553089141845703, P: -7.388272285461426\n",
      "\n",
      "Update #69, Avg Reward (E, I): 711.5, 17.694551467895508\n",
      "I: 0.16238872706890106, F: 3.538910388946533, P: -8.847275733947754\n",
      "\n",
      "Update #70, Avg Reward (E, I): 686.0, 19.301603317260742\n",
      "I: 0.04399462044239044, F: 3.860320806503296, P: -9.650801658630371\n",
      "\n",
      "Update #71, Avg Reward (E, I): 687.0, 19.1826114654541\n",
      "I: 0.007098187692463398, F: 3.836522340774536, P: -9.59130573272705\n",
      "\n",
      "Update #72, Avg Reward (E, I): 687.0, 17.89019012451172\n",
      "I: 0.0013404606143012643, F: 3.578037977218628, P: -8.94509506225586\n",
      "\n",
      "Update #73, Avg Reward (E, I): 687.0, 16.02907371520996\n",
      "I: 0.00048588216304779053, F: 3.205814838409424, P: -8.01453685760498\n",
      "\n",
      "Update #74, Avg Reward (E, I): 687.0, 13.972207069396973\n",
      "I: 0.00020155396487098187, F: 2.7944414615631104, P: -6.986103534698486\n",
      "\n",
      "Update #75, Avg Reward (E, I): 687.0, 11.878847122192383\n",
      "I: 9.288263390772045e-05, F: 2.3757693767547607, P: -5.939423561096191\n",
      "\n",
      "Update #76, Avg Reward (E, I): 687.0, 9.824579238891602\n",
      "I: 4.6847821067785844e-05, F: 1.9649158716201782, P: -4.912289619445801\n",
      "\n",
      "Update #77, Avg Reward (E, I): 687.0, 7.907881259918213\n",
      "I: 2.5357783670187928e-05, F: 1.5815762281417847, P: -3.9539406299591064\n",
      "\n",
      "Update #78, Avg Reward (E, I): 687.0, 6.228037357330322\n",
      "I: 1.45761532621691e-05, F: 1.2456074953079224, P: -3.114018678665161\n",
      "\n",
      "Update #79, Avg Reward (E, I): 687.0, 4.795323371887207\n",
      "I: 8.870812052919064e-06, F: 0.9590646624565125, P: -2.3976616859436035\n",
      "\n",
      "Update #80, Avg Reward (E, I): 687.0, 3.5694174766540527\n",
      "I: 5.586650786426617e-06, F: 0.7138835191726685, P: -1.7847087383270264\n",
      "\n",
      "Update #81, Avg Reward (E, I): 687.0, 2.5598607063293457\n",
      "I: 3.6958879263693234e-06, F: 0.5119721293449402, P: -1.2799303531646729\n",
      "\n",
      "Update #82, Avg Reward (E, I): 687.0, 1.8115991353988647\n",
      "I: 2.531509380787611e-06, F: 0.36231982707977295, P: -0.9057995676994324\n",
      "\n",
      "Update #83, Avg Reward (E, I): 687.0, 1.3092135190963745\n",
      "I: 1.9053621826969902e-06, F: 0.2618426978588104, P: -0.6546067595481873\n",
      "\n",
      "Update #84, Avg Reward (E, I): 687.0, 0.9758147597312927\n",
      "I: 1.4305115882962127e-06, F: 0.19516295194625854, P: -0.48790737986564636\n",
      "\n",
      "Update #85, Avg Reward (E, I): 687.0, 0.7592331171035767\n",
      "I: 9.536744300930877e-07, F: 0.15184663236141205, P: -0.37961655855178833\n",
      "\n",
      "Update #86, Avg Reward (E, I): 687.0, 0.6316853761672974\n",
      "I: 7.152557941481064e-07, F: 0.12633708119392395, P: -0.3158426880836487\n",
      "\n",
      "Update #87, Avg Reward (E, I): 687.0, 0.5581656098365784\n",
      "I: 6.198883397701138e-07, F: 0.11163312196731567, P: -0.2790828049182892\n",
      "\n",
      "Update #88, Avg Reward (E, I): 687.0, 0.5041601061820984\n",
      "I: 4.768372150465439e-07, F: 0.1008320227265358, P: -0.2520800530910492\n",
      "\n",
      "Update #89, Avg Reward (E, I): 687.0, 0.44947293400764465\n",
      "I: 3.8146978909026075e-07, F: 0.08989458531141281, P: -0.22473646700382233\n",
      "\n",
      "Update #90, Avg Reward (E, I): 687.0, 0.3899679183959961\n",
      "I: 3.8146978909026075e-07, F: 0.07799358665943146, P: -0.19498395919799805\n",
      "\n",
      "Update #91, Avg Reward (E, I): 687.0, 0.3310947120189667\n",
      "I: 2.861023347122682e-07, F: 0.06621894240379333, P: -0.16554735600948334\n",
      "\n",
      "Update #92, Avg Reward (E, I): 687.0, 0.28047096729278564\n",
      "I: 1.9073489454513037e-07, F: 0.05609419569373131, P: -0.14023548364639282\n",
      "\n",
      "Update #93, Avg Reward (E, I): 687.0, 0.24260355532169342\n",
      "I: 1.9073489454513037e-07, F: 0.048520710319280624, P: -0.12130177766084671\n",
      "\n",
      "Update #94, Avg Reward (E, I): 687.0, 0.21716804802417755\n",
      "I: 1.9073489454513037e-07, F: 0.04343361034989357, P: -0.10858402401208878\n",
      "\n",
      "Update #95, Avg Reward (E, I): 687.0, 0.20097774267196655\n",
      "I: 1.9073489454513037e-07, F: 0.04019555076956749, P: -0.10048887133598328\n",
      "\n",
      "Update #96, Avg Reward (E, I): 687.0, 0.1909639686346054\n",
      "I: 9.536743306171047e-08, F: 0.03819279372692108, P: -0.0954819843173027\n",
      "\n",
      "Update #97, Avg Reward (E, I): 687.0, 0.1851193755865097\n",
      "I: 9.536743306171047e-08, F: 0.03702387586236, P: -0.09255968779325485\n",
      "\n",
      "Update #98, Avg Reward (E, I): 687.0, 0.18146075308322906\n",
      "I: 9.536743306171047e-08, F: 0.03629215061664581, P: -0.09073037654161453\n",
      "\n",
      "Update #99, Avg Reward (E, I): 687.0, 0.17736555635929108\n",
      "I: 9.536743306171047e-08, F: 0.035473112016916275, P: -0.08868277817964554\n",
      "\n",
      "Update #100, Avg Reward (E, I): 687.0, 0.17045539617538452\n",
      "I: 9.536743306171047e-08, F: 0.03409108147025108, P: -0.08522769808769226\n",
      "\n",
      "Update #101, Avg Reward (E, I): 687.0, 0.15984204411506653\n",
      "I: 9.536743306171047e-08, F: 0.031968411058187485, P: -0.07992102205753326\n",
      "\n",
      "Update #102, Avg Reward (E, I): 687.0, 0.14645139873027802\n",
      "I: 9.536743306171047e-08, F: 0.029290279373526573, P: -0.07322569936513901\n",
      "\n",
      "Update #103, Avg Reward (E, I): 687.0, 0.13238635659217834\n",
      "I: 9.536743306171047e-08, F: 0.0264772716909647, P: -0.06619317829608917\n",
      "\n",
      "Update #104, Avg Reward (E, I): 687.0, 0.11986977607011795\n",
      "I: 9.536743306171047e-08, F: 0.02397395484149456, P: -0.059934888035058975\n",
      "\n",
      "Update #105, Avg Reward (E, I): 687.0, 0.11030889302492142\n",
      "I: 9.536743306171047e-08, F: 0.022061778232455254, P: -0.05515444651246071\n",
      "\n",
      "Update #106, Avg Reward (E, I): 687.0, 0.10383135825395584\n",
      "I: 9.536743306171047e-08, F: 0.02076627127826214, P: -0.05191567912697792\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2271964b3e0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrain_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mi_rew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Train the network with PPO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mupdate_i_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_rew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-56e838262acd>\u001b[0m in \u001b[0;36mupdate_func\u001b[0;34m(train_data, train_type)\u001b[0m\n\u001b[1;32m    136\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_holders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreshape_train_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_holders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                                                   self.next_obs_holders: reshape_train_var(train_data[:, 3])})\n\u001b[0m\u001b[1;32m    139\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mi_rew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "update_rewards = []\n",
    "update_i_rewards = []\n",
    "train_type = 0\n",
    "\n",
    "for i in range(int(n_episodes / batch_size)):\n",
    "    ec.sim_episodes(network, batch_size, max_steps) # Simualate env to generate data\n",
    "    update_rewards.append(ec.get_avg_reward()) # Append rewards to reward tracker list\n",
    "    dat = ec.get_data() # Get all the data gathered\n",
    "    \n",
    "#     if train_type == 0 and len(update_i_rewards) >= 4 and update_i_rewards[-1] <= update_i_rewards[-2] and \\\n",
    "#        update_i_rewards[-1] <= update_i_rewards[-3] and update_i_rewards[-1] <= update_i_rewards[-4]:\n",
    "#         print('Switching to PPO training')\n",
    "#         train_type = 1\n",
    "    \n",
    "    if train_type == 0:\n",
    "        i_rew, losses = network.train(dat, train_type=0) # Train the network with PPO\n",
    "        update_i_rewards.append(i_rew)\n",
    "\n",
    "        if i != 0 and i % print_freq == 0:\n",
    "            print(f'Update #{i}, Avg Reward (E, I): {np.mean(update_rewards[-print_freq:])}, ' + \\\n",
    "                  f'{np.mean(update_i_rewards[-print_freq:])}')\n",
    "            print(f'I: {losses[0]}, F: {losses[1]}, P: {losses[2]}')\n",
    "            print()\n",
    "    else:\n",
    "        network.train(dat, train_type=1) # Train the network with PPO\n",
    "\n",
    "        if i != 0 and i % print_freq == 0:\n",
    "            print(f'Update #{i}, Avg Reward (E): {np.mean(update_rewards[-print_freq:])}')\n",
    "        \n",
    "#     if i != 0 and i % (print_freq * 5) == 0:\n",
    "#         ec.render_episodes(network, 1, max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec.render_episodes(network, 5, max_steps) # Render an episode to see the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Breakout Example\n",
    "\n",
    "### Atari Breakout\n",
    "\n",
    "Please do note that this example may take a long time to train.\n",
    "\n",
    "With the default 4 threads runnning on an 8-core CPU with a GTX 1080 Ti, it will take several hours to train to a decent level of play.\n",
    "\n",
    "Running on a platform with more GPU power and a larger cluster of CPUs could siginificantly reduce training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.backend import categorical_crossentropy\n",
    "from ludus.policies import BaseTrainer\n",
    "from ludus.env import EnvController\n",
    "from ludus.utils import preprocess_atari, reshape_train_var\n",
    "import gym\n",
    "# Super Mario stuff\n",
    "from nes_py.wrappers import BinarySpaceToDiscreteSpaceEnv\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "    env = BinarySpaceToDiscreteSpaceEnv(env, SIMPLE_MOVEMENT)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMTrainer(BaseTrainer):\n",
    "    def __init__(self, in_op, out_op, value_out_op, act_type='discrete', sess=None, clip_val=0.2, ppo_iters=80,\n",
    "                 target_kl=0.01, v_coef=1., entropy_coef=0.01):\n",
    "        self.value_out_op = value_out_op\n",
    "        self.clip_val = clip_val\n",
    "        self.ppo_iters = ppo_iters\n",
    "        self.target_kl = target_kl\n",
    "        self.v_coef = v_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        # ICM parameters, TODO: make parameters for these\n",
    "        self.ro_coef = 0.1\n",
    "        self.beta = 0.2\n",
    "        self.eta = 1\n",
    "        self.r_i_coef = 1\n",
    "        self.r_e_coef = 0.05\n",
    "        \n",
    "        super().__init__(in_op, out_op, act_type, sess)\n",
    "        \n",
    "    def _create_ICM(self, optimizer=tf.train.AdamOptimizer()):\n",
    "        feature_dim = 64 # TODO: make a parameter for this\n",
    "        \n",
    "        # Create placeholder\n",
    "        self.next_obs_holders = tf.placeholder(tf.float32, shape=self.in_op.shape)\n",
    "        \n",
    "        # Observation feature encoder\n",
    "        with tf.variable_scope('feature_encoder'):\n",
    "            enc_layers = [\n",
    "                Conv2D(16, 4, (2, 2), activation=tf.nn.relu, name='fe_conv'),\n",
    "                MaxPool2D(2, name='fe_max_pool'),\n",
    "                Conv2D(16, 3, activation=tf.nn.relu, name='fe_conv2'),\n",
    "                MaxPool2D(2, name='fe_max_pool2'),\n",
    "                Conv2D(32, 3, activation=tf.nn.relu, name='fe_conv3'),\n",
    "                MaxPool2D(2, name='fe_max_pool3'),\n",
    "                Flatten(name='fe_flattened'),\n",
    "                Dense(feature_dim, activation=tf.nn.relu, name='fe_dense')\n",
    "            ]\n",
    "            \n",
    "            # Encoding state\n",
    "            self.f_obs = enc_layers[0](self.in_op)\n",
    "            for i in range(1, len(enc_layers)):\n",
    "                self.f_obs = enc_layers[i](self.f_obs)\n",
    "            \n",
    "            # Encoding the next state\n",
    "            self.f_obs_next = enc_layers[0](self.next_obs_holders)\n",
    "            for i in range(1, len(enc_layers)):\n",
    "                self.f_obs_next = enc_layers[i](self.f_obs_next)\n",
    "            \n",
    "        # State predictor forward model\n",
    "        with tf.variable_scope('forward_model'):\n",
    "            self.state_act_pair = tf.concat([self.out_op, self.f_obs], axis=1)\n",
    "            self.sp_dense = Dense(64, activation=tf.nn.relu)(self.state_act_pair)\n",
    "            self.f_obs_next_pred = Dense(feature_dim, activation=tf.nn.relu)(self.sp_dense)\n",
    "        \n",
    "        # Inverse dynamics model (predicting action)\n",
    "        with tf.variable_scope('inverse_model'):\n",
    "            self.state_state_pair = tf.concat([self.f_obs, self.f_obs_next], axis=1)\n",
    "            self.act_preds = Dense(64, activation=tf.nn.relu)(self.state_state_pair)\n",
    "            # TODO: softmax only works for discrete\n",
    "            self.act_preds = Dense(self.out_op.shape[1], activation=tf.nn.softmax)(self.act_preds)\n",
    "        \n",
    "        # Calculating intrinsic reward\n",
    "        self.obs_pred_diff = self.f_obs_next_pred - self.f_obs_next\n",
    "        self.r_i = 0.5 * self.eta * tf.reduce_sum(self.obs_pred_diff ** 2, axis=1) # Fix these squares (Probably okay)\n",
    "        self.r_ie = self.r_i_coef * self.r_i + self.r_e_coef * self.reward_holders\n",
    "        \n",
    "        # Calculating losses\n",
    "        self.pre_loss_i = categorical_crossentropy(self.act_masks, self.act_preds) # tf.reduce_sum((self.act_holders - self.act_pred) ** 2, axis=1)\n",
    "        self.pre_loss_f = 0.5 * tf.reduce_sum(self.obs_pred_diff ** 2, axis=1)\n",
    "        \n",
    "        self.loss_i = (1 - self.beta) * tf.reduce_mean(self.pre_loss_i)\n",
    "        self.loss_f = self.beta * tf.reduce_mean(self.pre_loss_f)\n",
    "        self.loss_p = -self.ro_coef * tf.reduce_sum(self.r_ie)\n",
    "        \n",
    "        # Making update functions\n",
    "        self.i_train_vars = tf.trainable_variables(scope='feature_encoder') + tf.trainable_variables(scope='inverse_model')\n",
    "        self.f_train_vars = tf.trainable_variables(scope='forward_model')\n",
    "        self.p_train_vars = [var for var in tf.trainable_variables() if var not in (self.i_train_vars + self.f_train_vars)]\n",
    "        \n",
    "        self.li_update = optimizer.minimize(self.loss_i, var_list=self.i_train_vars)\n",
    "        self.lf_update = optimizer.minimize(self.loss_f, var_list=self.f_train_vars)\n",
    "        self.lp_update = optimizer.minimize(self.loss_p, var_list=self.p_train_vars)\n",
    "        \n",
    "        self.icm_updates = [self.li_update, self.lf_update, self.lp_update]\n",
    "        \n",
    "    def _create_discrete_trainer(self, optimizer=tf.train.AdamOptimizer()):\n",
    "        \"\"\"\n",
    "        Creates a function for vanilla policy training with a discrete action space\n",
    "        \"\"\"\n",
    "        # First passthrough\n",
    "        \n",
    "        self.act_holders = tf.placeholder(tf.int32, shape=[None])\n",
    "        self.reward_holders = tf.placeholder(tf.float32, shape=[None])\n",
    "        \n",
    "        self.act_masks = tf.one_hot(self.act_holders, self.out_op.shape[1].value, dtype=tf.float32)\n",
    "        self.resp_acts = tf.reduce_sum(self.act_masks *  self.out_op, axis=1)\n",
    "        \n",
    "        self.advantages = self.reward_holders - tf.squeeze(self.value_out_op)\n",
    "        \n",
    "        self._create_ICM()\n",
    "        \n",
    "        # Second passthrough\n",
    "        \n",
    "        self.advatange_holders = tf.placeholder(dtype=tf.float32, shape=self.advantages.shape)\n",
    "        self.old_prob_holders = tf.placeholder(dtype=tf.float32, shape=self.resp_acts.shape)\n",
    " \n",
    "        self.policy_ratio = self.resp_acts / self.old_prob_holders\n",
    "        self.clipped_ratio = tf.clip_by_value(self.policy_ratio, 1 - self.clip_val, 1 + self.clip_val)\n",
    "\n",
    "        self.min_loss = tf.minimum(self.policy_ratio * self.advatange_holders, self.clipped_ratio * self.advatange_holders)\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "        # Actor update\n",
    "        \n",
    "        self.kl_divergence = tf.reduce_mean(tf.log(self.old_prob_holders) - tf.log(self.resp_acts))\n",
    "        self.actor_loss = -tf.reduce_mean(self.min_loss)\n",
    "        self.actor_update = self.optimizer.minimize(self.actor_loss)\n",
    "\n",
    "        # Value update\n",
    "        \n",
    "        self.value_loss = tf.reduce_mean(tf.square(self.reward_holders - tf.squeeze(self.value_out_op)))\n",
    "        self.value_update = self.optimizer.minimize(self.value_loss)\n",
    "        \n",
    "        # Intrinsic motivation update\n",
    "        \n",
    "        self.intrinsic_loss = self.loss_i + self.loss_f\n",
    "        self.intrinsic_update = self.optimizer.minimize(self.intrinsic_loss)\n",
    "        \n",
    "        # Combined update\n",
    "        \n",
    "        self.entropy = -tf.reduce_mean(tf.reduce_sum(self.out_op * tf.log(1. / tf.clip_by_value(self.out_op, 1e-8, 1.0)), axis=1))\n",
    "        self.combined_loss = self.actor_loss + self.v_coef * self.value_loss + self.entropy_coef * self.entropy\n",
    "        self.combined_update = self.optimizer.minimize(self.combined_loss)\n",
    "        \n",
    "        def update_func(train_data):\n",
    "            i_rew, _, _, _ = self.sess.run([self.r_i] + self.icm_updates, \n",
    "                                   feed_dict={self.in_op: reshape_train_var(train_data[:, 0]),\n",
    "                                              self.act_holders: reshape_train_var(train_data[:, 1]),\n",
    "                                              self.reward_holders: train_data[:, 2],\n",
    "                                              self.next_obs_holders: reshape_train_var(train_data[:, 3])})\n",
    "            \n",
    "#             self.old_probs, self.old_advantages = self.sess.run([self.resp_acts, self.advantages], \n",
    "#                                     feed_dict={self.in_op: reshape_train_var(train_data[:, 0]),\n",
    "#                                                self.act_holders: train_data[:, 1],\n",
    "#                                                self.reward_holders: train_data[:, 2]})\n",
    "    \n",
    "#             for i in range(self.ppo_iters):\n",
    "#                 kl_div, i_rew, _ = self.sess.run([self.kl_divergence, self.r_i, self.intrinsic_loss], \n",
    "#                                    feed_dict={self.in_op: reshape_train_var(train_data[:, 0]),\n",
    "#                                         self.act_holders: reshape_train_var(train_data[:, 1]),\n",
    "#                                         self.reward_holders: train_data[:, 2],\n",
    "#                                         self.old_prob_holders: self.old_probs,\n",
    "#                                         self.advatange_holders: self.old_advantages})\n",
    "#                 i_rews.append(i_rew)\n",
    "#                 if kl_div > 1.5 * self.target_kl:\n",
    "#                     break    \n",
    "    \n",
    "            return i_rew\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        return update_func\n",
    "        \n",
    "    def _create_continuous_trainer(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ejmejm/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = make_env() # This instance of the environment is only used\n",
    "                              # to get action dimensions\n",
    "in_shape = [84, 84, 1] # Size of reshaped observations\n",
    "\n",
    "# Creating a conv net for the policy and value estimator\n",
    "obs_op = Input(shape=in_shape)\n",
    "conv1 = Conv2D(16, 8, (4, 4), activation='relu')(obs_op)\n",
    "max_pool1 = MaxPool2D(2, 2)(conv1)\n",
    "conv2 = Conv2D(32, 4, (2, 2), activation='relu')(max_pool1)\n",
    "max_pool2 = MaxPool2D(2, 2)(conv2)\n",
    "dense1 = Dense(256, activation='relu')(max_pool2)\n",
    "flattened = Flatten()(dense1)\n",
    "\n",
    "# Output probability distribution over possible actions\n",
    "act_probs_op = Dense(env.action_space.n, activation='softmax')(flattened)\n",
    "\n",
    "# Output value of observed state\n",
    "value_op = Dense(1)(flattened)\n",
    "\n",
    "# Wrap a Proximal Policy Optimization Trainer on top of the network\n",
    "network = IMTrainer(obs_op, act_probs_op, value_op, act_type='discrete', ppo_iters=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10000 # Total episodes of data to collect\n",
    "max_steps = 2048 # Max number of frames per game\n",
    "batch_size = 4 # Smaller = faster, larger = stabler\n",
    "print_freq = 1 # How many training updates between printing progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment controller for generating game data\n",
    "ec = EnvController(make_env, n_threads=4)\n",
    "# Set the preprocessing function for observations\n",
    "ec.set_obs_transform(lambda x: preprocess_atari(x.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update #1, Avg Reward (E, I): 570.5, 0.025459524244070053\n",
      "Update #2, Avg Reward (E, I): 481.75, 0.019870921969413757\n",
      "Update #3, Avg Reward (E, I): 571.5, 0.017639750614762306\n",
      "Update #4, Avg Reward (E, I): 448.5, 0.018466830253601074\n",
      "Update #5, Avg Reward (E, I): 509.0, 0.02011704072356224\n",
      "Update #6, Avg Reward (E, I): 530.5, 0.02301792800426483\n",
      "Update #7, Avg Reward (E, I): 445.75, 0.02790858969092369\n",
      "Update #8, Avg Reward (E, I): 407.0, 0.038784295320510864\n",
      "Update #9, Avg Reward (E, I): 257.75, 0.060099150985479355\n",
      "Update #10, Avg Reward (E, I): 189.25, 0.09475848823785782\n",
      "Update #11, Avg Reward (E, I): 361.25, 0.15753334760665894\n",
      "Update #12, Avg Reward (E, I): 432.0, 0.24771468341350555\n",
      "Update #13, Avg Reward (E, I): 369.5, 0.4025205969810486\n",
      "Update #14, Avg Reward (E, I): 449.0, 0.625564694404602\n",
      "Update #15, Avg Reward (E, I): 447.25, 0.9790916442871094\n",
      "Update #16, Avg Reward (E, I): 576.5, 1.472145915031433\n",
      "Update #17, Avg Reward (E, I): 480.25, 2.0971384048461914\n",
      "Update #18, Avg Reward (E, I): 604.5, 2.823261260986328\n",
      "Update #19, Avg Reward (E, I): 194.25, 3.4326014518737793\n",
      "Update #20, Avg Reward (E, I): 12.75, 4.44317626953125\n",
      "Update #21, Avg Reward (E, I): -47.5, 5.982297897338867\n",
      "Update #22, Avg Reward (E, I): -79.0, 8.379256248474121\n",
      "Update #23, Avg Reward (E, I): -79.5, 11.933324813842773\n",
      "Update #24, Avg Reward (E, I): -80.75, 17.14914894104004\n",
      "Update #25, Avg Reward (E, I): -53.75, 23.817054748535156\n",
      "Update #26, Avg Reward (E, I): -24.25, 31.09610366821289\n",
      "Update #27, Avg Reward (E, I): 76.5, 36.52503204345703\n",
      "Update #28, Avg Reward (E, I): 635.0, 38.05799102783203\n",
      "Update #29, Avg Reward (E, I): 548.0, 31.32208251953125\n",
      "Update #30, Avg Reward (E, I): 508.75, 22.975353240966797\n",
      "Update #31, Avg Reward (E, I): 576.5, 14.943840026855469\n",
      "Update #32, Avg Reward (E, I): 573.0, 9.056784629821777\n",
      "Update #33, Avg Reward (E, I): 510.25, 5.379495620727539\n",
      "Update #34, Avg Reward (E, I): 533.5, 2.9625911712646484\n",
      "Update #35, Avg Reward (E, I): 510.0, 1.7243072986602783\n",
      "Update #36, Avg Reward (E, I): 707.5, 1.0247482061386108\n",
      "Update #37, Avg Reward (E, I): 550.0, 0.7096245884895325\n",
      "Update #38, Avg Reward (E, I): 708.75, 0.5182613730430603\n",
      "Update #39, Avg Reward (E, I): 687.0, 0.43160802125930786\n",
      "Update #40, Avg Reward (E, I): 686.25, 0.41293054819107056\n",
      "Update #41, Avg Reward (E, I): 687.0, 0.4264611601829529\n",
      "Update #42, Avg Reward (E, I): 708.75, 0.4832928776741028\n",
      "Update #43, Avg Reward (E, I): 686.25, 0.5100982785224915\n",
      "Update #44, Avg Reward (E, I): 687.0, 0.5725579857826233\n",
      "Update #45, Avg Reward (E, I): 687.0, 0.6547362804412842\n",
      "Update #46, Avg Reward (E, I): 687.0, 0.7629253268241882\n",
      "Update #47, Avg Reward (E, I): 687.0, 0.9028171300888062\n",
      "Update #48, Avg Reward (E, I): 687.0, 1.0884099006652832\n"
     ]
    }
   ],
   "source": [
    "update_rewards = []\n",
    "update_i_rewards = []\n",
    "\n",
    "for i in range(int(n_episodes / batch_size)):\n",
    "    ec.sim_episodes(network, batch_size, max_steps) # Simualate env to generate data\n",
    "    update_rewards.append(ec.get_avg_reward()) # Append rewards to reward tracker list\n",
    "    dat = ec.get_data() # Get all the data gathered\n",
    "    i_rew = network.train(dat) # Train the network with PPO\n",
    "    update_i_rewards.append(i_rew)\n",
    "    if i != 0 and i % print_freq == 0:\n",
    "        print(f'Update #{i}, Avg Reward (E, I): {np.mean(update_rewards[-print_freq:])}, {np.mean(update_i_rewards[-print_freq:])}') # Print an update\n",
    "    if i != 0 and i % (print_freq * 5) == 0:\n",
    "        ec.render_episodes(network, 5, max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec.render_episodes(network, 5, max_steps) # Render an episode to see the result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

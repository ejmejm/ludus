{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Breakout Example\n",
    "\n",
    "### Atari Breakout\n",
    "\n",
    "Please do note that this example may take a long time to train.\n",
    "\n",
    "With the default 4 threads runnning on an 8-core CPU with a GTX 1080 Ti, it will take several hours to train to a decent level of play.\n",
    "\n",
    "Running on a platform with more GPU power and a larger cluster of CPUs could siginificantly reduce training time.\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1705.05363.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.backend import categorical_crossentropy\n",
    "from ludus.policies import BaseTrainer\n",
    "from ludus.env import EnvController\n",
    "from ludus.utils import preprocess_atari, reshape_train_var\n",
    "from ludus.memory import MTMemoryBuffer\n",
    "import gym\n",
    "# Super Mario stuff\n",
    "from nes_py.wrappers import BinarySpaceToDiscreteSpaceEnv\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "    env = BinarySpaceToDiscreteSpaceEnv(env, SIMPLE_MOVEMENT)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMTrainer(BaseTrainer):\n",
    "    def __init__(self, in_op, out_op, value_out_op, act_type='discrete', sess=None, clip_val=0.2, ppo_iters=80,\n",
    "                 target_kl=0.01, v_coef=1., entropy_coef=0.01):\n",
    "        self.value_out_op = value_out_op\n",
    "        self.clip_val = clip_val\n",
    "        self.ppo_iters = ppo_iters\n",
    "        self.target_kl = target_kl\n",
    "        self.v_coef = v_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        # ICM parameters, TODO: make parameters for these\n",
    "        self.ro_coef = 0.5\n",
    "        self.beta = 0.2\n",
    "        self.eta = 1\n",
    "        self.r_i_coef = 1\n",
    "        self.r_e_coef = 0\n",
    "        \n",
    "        super().__init__(in_op, out_op, act_type, sess)\n",
    "        \n",
    "    def _create_ICM(self, optimizer=tf.train.AdamOptimizer()):\n",
    "        feature_dim = 128 # TODO: make a parameter for this\n",
    "        \n",
    "        # Create placeholder\n",
    "        self.next_obs_holders = tf.placeholder(tf.float32, shape=self.in_op.shape)\n",
    "        \n",
    "        # Observation feature encoder\n",
    "        with tf.variable_scope('feature_encoder'):\n",
    "            enc_layers = [\n",
    "                Conv2D(32, 3, activation=tf.nn.tanh, name='fe_conv'),\n",
    "                MaxPool2D(2, name='fe_max_pool'),\n",
    "                Conv2D(32, 3, activation=tf.nn.tanh, name='fe_conv2'),\n",
    "                MaxPool2D(2, name='fe_max_pool2'),\n",
    "                Conv2D(32, 3, activation=tf.nn.tanh, name='fe_conv3'),\n",
    "                MaxPool2D(2, name='fe_max_pool3'),\n",
    "                Flatten(name='fe_flattened'),\n",
    "                Dense(feature_dim, activation=tf.nn.tanh, use_bias=False, name='fe_dense')\n",
    "            ]\n",
    "            \n",
    "            # Encoding state\n",
    "            self.f_obs = enc_layers[0](self.in_op)\n",
    "            for i in range(1, len(enc_layers)):\n",
    "                self.f_obs = enc_layers[i](self.f_obs)\n",
    "            \n",
    "            # Encoding the next state\n",
    "            self.f_obs_next = enc_layers[0](self.next_obs_holders)\n",
    "            for i in range(1, len(enc_layers)):\n",
    "                self.f_obs_next = enc_layers[i](self.f_obs_next)\n",
    "            \n",
    "        # State predictor forward model\n",
    "        with tf.variable_scope('forward_model'):\n",
    "            self.state_act_pair = tf.concat([self.out_op, self.f_obs], axis=1)\n",
    "            self.sp_dense = Dense(64, activation=tf.nn.tanh)(self.state_act_pair)\n",
    "            self.f_obs_next_pred = Dense(feature_dim, activation=tf.nn.tanh, use_bias=False)(self.sp_dense)\n",
    "        \n",
    "        # Inverse dynamics model (predicting action)\n",
    "        with tf.variable_scope('inverse_model'):\n",
    "            self.state_state_pair = tf.concat([self.f_obs, self.f_obs_next], axis=1)\n",
    "            self.act_preds = Dense(64, activation=tf.nn.relu)(self.state_state_pair)\n",
    "            # TODO: softmax only works for discrete\n",
    "            self.act_preds = Dense(self.out_op.shape[1], use_bias=False, activation=tf.nn.softmax)(self.act_preds)\n",
    "        \n",
    "        # Calculating intrinsic reward\n",
    "        self.obs_pred_diff = self.f_obs_next_pred - self.f_obs_next\n",
    "        self.r_i = 0.5 * self.eta * tf.reduce_sum(self.obs_pred_diff ** 2, axis=1) # Fix these squares (Probably okay)\n",
    "        self.r_ie = self.r_i_coef * self.r_i # + self.r_e_coef * self.reward_holders\n",
    "        \n",
    "        # Calculating losses\n",
    "        self.pre_loss_i = categorical_crossentropy(self.act_masks, self.act_preds) # tf.reduce_sum((self.act_holders - self.act_pred) ** 2, axis=1)\n",
    "        self.pre_loss_f = 0.5 * tf.reduce_sum(self.obs_pred_diff ** 2, axis=1)\n",
    "        \n",
    "        self.loss_i = (1 - self.beta) * tf.reduce_mean(self.pre_loss_i)\n",
    "        self.loss_f = self.beta * tf.reduce_mean(self.pre_loss_f)\n",
    "        self.loss_p = -self.ro_coef * tf.reduce_sum(self.r_ie)\n",
    "        \n",
    "        # Making update functions\n",
    "        self.i_train_vars = tf.trainable_variables(scope='feature_encoder') + tf.trainable_variables(scope='inverse_model')\n",
    "        self.f_train_vars = tf.trainable_variables(scope='forward_model')\n",
    "        self.p_train_vars = [var for var in tf.trainable_variables() if var not in (self.i_train_vars + self.f_train_vars)]\n",
    "        \n",
    "        self.li_update = optimizer.minimize(self.loss_i, var_list=self.i_train_vars)\n",
    "        self.lf_update = optimizer.minimize(self.loss_f, var_list=self.f_train_vars)\n",
    "        self.lp_update = optimizer.minimize(self.loss_p, var_list=self.p_train_vars)\n",
    "        \n",
    "        self.icm_updates = [self.li_update, self.lf_update, self.lp_update]\n",
    "        \n",
    "    def _create_discrete_trainer(self, optimizer=tf.train.AdamOptimizer()):\n",
    "        \"\"\"\n",
    "        Creates a function for vanilla policy training with a discrete action space\n",
    "        \"\"\"\n",
    "        # First passthrough\n",
    "        \n",
    "        self.act_holders = tf.placeholder(tf.int32, shape=[None])\n",
    "        self.reward_holders = tf.placeholder(tf.float32, shape=[None])\n",
    "        \n",
    "        self.act_masks = tf.one_hot(self.act_holders, self.out_op.shape[1].value, dtype=tf.float32)\n",
    "        self.resp_acts = tf.reduce_sum(self.act_masks *  self.out_op, axis=1)\n",
    "        \n",
    "        self.advantages = self.reward_holders - tf.squeeze(self.value_out_op)\n",
    "        \n",
    "        self._create_ICM()\n",
    "        \n",
    "        # Second passthrough\n",
    "        \n",
    "        self.advatange_holders = tf.placeholder(dtype=tf.float32, shape=self.advantages.shape)\n",
    "        self.old_prob_holders = tf.placeholder(dtype=tf.float32, shape=self.resp_acts.shape)\n",
    " \n",
    "        self.policy_ratio = self.resp_acts / self.old_prob_holders\n",
    "        self.clipped_ratio = tf.clip_by_value(self.policy_ratio, 1 - self.clip_val, 1 + self.clip_val)\n",
    "\n",
    "        self.min_loss = tf.minimum(self.policy_ratio * self.advatange_holders, self.clipped_ratio * self.advatange_holders)\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "        # Actor update\n",
    "        \n",
    "        self.kl_divergence = tf.reduce_mean(tf.log(self.old_prob_holders) - tf.log(self.resp_acts))\n",
    "        self.actor_loss = -tf.reduce_mean(self.min_loss)\n",
    "        self.actor_update = self.optimizer.minimize(self.actor_loss)\n",
    "\n",
    "        # Value update\n",
    "        \n",
    "        self.value_loss = tf.reduce_mean(tf.square(self.reward_holders - tf.squeeze(self.value_out_op)))\n",
    "        self.value_update = self.optimizer.minimize(self.value_loss)\n",
    "        \n",
    "        # Combined update\n",
    "        \n",
    "        self.entropy = -tf.reduce_mean(tf.reduce_sum(self.out_op * tf.log(1. / tf.clip_by_value(self.out_op, 1e-8, 1.0)), axis=1))\n",
    "        self.combined_loss = self.actor_loss + self.v_coef * self.value_loss + self.entropy_coef * self.entropy\n",
    "        self.combined_update = self.optimizer.minimize(self.combined_loss)\n",
    "        \n",
    "        def update_func(train_data, train_type=0):\n",
    "            if train_type == 0:\n",
    "                i_rew, _, _, _ = self.sess.run([tf.reduce_mean(self.r_i)] + self.icm_updates, \n",
    "                                       feed_dict={self.in_op: reshape_train_var(train_data[:, 0]),\n",
    "                                                  self.act_holders: reshape_train_var(train_data[:, 1]),\n",
    "                                                  self.reward_holders: train_data[:, 2],\n",
    "                                                  self.next_obs_holders: reshape_train_var(train_data[:, 3])})\n",
    "                return i_rew\n",
    "            else:\n",
    "                self.old_probs, self.old_advantages = self.sess.run([self.resp_acts, self.advantages], \n",
    "                                        feed_dict={self.in_op: reshape_train_var(train_data[:, 0]),\n",
    "                                                   self.act_holders: train_data[:, 1],\n",
    "                                                   self.reward_holders: train_data[:, 2]})\n",
    "\n",
    "                for i in range(self.ppo_iters):\n",
    "                    kl_div, _ = self.sess.run([self.kl_divergence, self.combined_update], \n",
    "                                   feed_dict={self.in_op: reshape_train_var(train_data[:, 0]),\n",
    "                                        self.act_holders: reshape_train_var(train_data[:, 1]),\n",
    "                                        self.reward_holders: train_data[:, 2],\n",
    "                                        self.old_prob_holders: self.old_probs,\n",
    "                                        self.advatange_holders: self.old_advantages})\n",
    "                    if kl_div > 1.5 * self.target_kl:\n",
    "                        break\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        return update_func\n",
    "        \n",
    "    def _create_continuous_trainer(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ejmejm/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = make_env() # This instance of the environment is only used\n",
    "                              # to get action dimensions\n",
    "in_shape = [42, 42, 4] # Size of reshaped observations\n",
    "\n",
    "# Creating a conv net for the policy and value estimator\n",
    "obs_op = Input(shape=in_shape)\n",
    "conv1 = Conv2D(32, 3, activation='relu')(obs_op)\n",
    "max_pool1 = MaxPool2D(2, 2)(conv1)\n",
    "conv2 = Conv2D(32, 3, activation='relu')(max_pool1)\n",
    "max_pool2 = MaxPool2D(2, 2)(conv2)\n",
    "conv3 = Conv2D(32, 3, activation='relu')(max_pool2)\n",
    "max_pool3 = MaxPool2D(2, 2)(conv3)\n",
    "flattened = Flatten()(max_pool3)\n",
    "dense1 = Dense(128, activation='relu')(flattened)\n",
    "dense2 = Dense(256, activation='relu')(dense1)\n",
    "dense3 = Dense(128, activation='relu')(dense1)\n",
    "\n",
    "# Output probability distribution over possible actions\n",
    "act_probs_op = Dense(env.action_space.n, activation='softmax')(dense2)\n",
    "\n",
    "# Output value of observed state\n",
    "value_op = Dense(1)(dense3)\n",
    "\n",
    "# Wrap a Proximal Policy Optimization Trainer on top of the network\n",
    "network = IMTrainer(obs_op, act_probs_op, value_op, act_type='discrete', ppo_iters=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10000 # Total episodes of data to collect\n",
    "max_steps = 1024 # Max number of frames per game\n",
    "batch_size = 4 # Smaller = faster, larger = stabler\n",
    "print_freq = 1 # How many training updates between printing progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_hist = {} # Keeps track of up to 3 previous frames for each agent\n",
    "\n",
    "# Create observation transformation that adds the two last frames on\n",
    "# as two extra dimensions\n",
    "def new_obs_transform(obs, agent_id):\n",
    "    new_frame = preprocess_atari(obs.squeeze(), size=(42, 42)) # First preprocess the new frame\n",
    "    \n",
    "    if agent_id in agent_hist: # Case for a continued episode\n",
    "        agent_hist[agent_id] = agent_hist[agent_id][1:]\n",
    "        agent_hist[agent_id].append(new_frame)\n",
    "    else: # Case for a new episode\n",
    "        agent_hist[agent_id] = [new_frame, new_frame, new_frame, new_frame]\n",
    "    \n",
    "    # Format the data\n",
    "    arr = np.array(agent_hist[agent_id])\n",
    "    return np.swapaxes(arr, 0, 3).squeeze()\n",
    "\n",
    "############################################################\n",
    "############################################################\n",
    "\n",
    "mtmb = MTMemoryBuffer() # Create a memory buffer to store the episode data\n",
    "\n",
    "# Edit the memory buffer's start_rollout function so that every time\n",
    "# an episode ends, it resets the respective agent's history\n",
    "old_start_rollout = mtmb.start_rollout\n",
    "\n",
    "def new_start_rollout(agent_id):\n",
    "    old_start_rollout(agent_id)\n",
    "    agent_hist.pop(agent_id, None)\n",
    "    \n",
    "mtmb.start_rollout = new_start_rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment controller for generating game data\n",
    "ec = EnvController(make_env, n_threads=4, memory_buffer=mtmb)\n",
    "# Set the preprocessing function for observations\n",
    "ec.set_obs_transform(new_obs_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update #1, Avg Reward (E, I): 630.75, 3.378314971923828\n",
      "Update #2, Avg Reward (E, I): 564.5, 3.423107862472534\n",
      "Update #3, Avg Reward (E, I): 501.5, 3.3466954231262207\n",
      "Update #4, Avg Reward (E, I): 679.75, 3.3958358764648438\n",
      "Update #5, Avg Reward (E, I): 586.0, 3.4757235050201416\n",
      "Update #6, Avg Reward (E, I): 500.75, 3.595341444015503\n",
      "Update #7, Avg Reward (E, I): 606.5, 3.9837584495544434\n",
      "Update #8, Avg Reward (E, I): 523.0, 4.829998970031738\n",
      "Update #9, Avg Reward (E, I): 584.0, 6.370082855224609\n",
      "Update #10, Avg Reward (E, I): 578.25, 8.805055618286133\n",
      "Update #11, Avg Reward (E, I): 625.0, 12.41203498840332\n",
      "Update #12, Avg Reward (E, I): 588.75, 16.59143829345703\n",
      "Update #13, Avg Reward (E, I): 686.75, 20.203035354614258\n",
      "Update #14, Avg Reward (E, I): 687.0, 22.831180572509766\n",
      "Update #15, Avg Reward (E, I): 687.0, 24.219175338745117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3265, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-91c5fb59f5e3>\", line 29, in <module>\n",
      "    ec.render_episodes(network, 1, max_steps)\n",
      "  File \"/home/ejmejm/MLProjects/ludus/ludus/env.py\", line 210, in render_episodes\n",
      "    time.sleep(0.02)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2016, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "update_rewards = []\n",
    "update_i_rewards = []\n",
    "train_type = 0\n",
    "\n",
    "for i in range(int(n_episodes / batch_size)):\n",
    "    ec.sim_episodes(network, batch_size, max_steps) # Simualate env to generate data\n",
    "    update_rewards.append(ec.get_avg_reward()) # Append rewards to reward tracker list\n",
    "    dat = ec.get_data() # Get all the data gathered\n",
    "    \n",
    "#     if train_type == 0 and len(update_i_rewards) >= 4 and update_i_rewards[-1] <= update_i_rewards[-2] and \\\n",
    "#        update_i_rewards[-1] <= update_i_rewards[-3] and update_i_rewards[-1] <= update_i_rewards[-4]:\n",
    "#         print('Switching to PPO training')\n",
    "#         train_type = 1\n",
    "    \n",
    "    if train_type == 0:\n",
    "        i_rew = network.train(dat, train_type=0) # Train the network with PPO\n",
    "        update_i_rewards.append(i_rew)\n",
    "\n",
    "        if i != 0 and i % print_freq == 0:\n",
    "            print(f'Update #{i}, Avg Reward (E, I): {np.mean(update_rewards[-print_freq:])}, ' + \\\n",
    "                  f'{np.mean(update_i_rewards[-print_freq:])}')\n",
    "    else:\n",
    "        network.train(dat, train_type=1) # Train the network with PPO\n",
    "\n",
    "        if i != 0 and i % print_freq == 0:\n",
    "            print(f'Update #{i}, Avg Reward (E): {np.mean(update_rewards[-print_freq:])}')\n",
    "        \n",
    "    if i != 0 and i % (print_freq * 5) == 0:\n",
    "        ec.render_episodes(network, 1, max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec.render_episodes(network, 5, max_steps) # Render an episode to see the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Breakout Example\n",
    "\n",
    "### Atari Breakout\n",
    "\n",
    "Please do note that this example may take a long time to train.\n",
    "\n",
    "With the default 4 threads runnning on an 8-core CPU with a GTX 1080 Ti, it will take several hours to train to a decent level of play.\n",
    "\n",
    "Running on a platform with more GPU power and a larger cluster of CPUs could siginificantly reduce training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPool2D, Flatten, LSTM\n",
    "import gym\n",
    "from ludus.policies import PPOTrainer\n",
    "from ludus.env import EnvController\n",
    "from ludus.utils import preprocess_atari, reshape_train_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ejmejm/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v0') # This instance of the environment is only used\n",
    "                              # to get action dimensions\n",
    "in_shape = [84, 84, 1] # Size of reshaped observations\n",
    "\n",
    "# Creating a conv net for the policy and value estimator\n",
    "obs_op = Input(shape=in_shape)\n",
    "conv1 = Conv2D(16, 8, (4, 4), activation='relu')(obs_op)\n",
    "max_pool1 = MaxPool2D(2, 2)(conv1)\n",
    "conv2 = Conv2D(32, 4, (2, 2), activation='relu')(max_pool1)\n",
    "max_pool2 = MaxPool2D(2, 2)(conv2)\n",
    "flattened = Flatten()(max_pool2)\n",
    "dense1 = Dense(256, activation='relu')(flattened)\n",
    "\n",
    "# Output probability distribution over possible actions\n",
    "act_probs_op = Dense(env.action_space.n, activation='softmax')(flattened)\n",
    "\n",
    "# Output value of observed state\n",
    "value_op = Dense(1)(flattened)\n",
    "\n",
    "# Wrap a Proximal Policy Optimization Trainer on top of the network\n",
    "network = PPOTrainer(obs_op, act_probs_op, value_op, act_type='discrete', ppo_iters=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10000 # Total episodes of data to collect\n",
    "max_steps = 2048 # Max number of frames per game\n",
    "batch_size = 8 # Smaller = faster, larger = stabler\n",
    "print_freq = 10 # How many training updates between printing progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment controller for generating game data\n",
    "ec = EnvController(lambda: gym.make('Breakout-v0'), n_threads=4)\n",
    "# Set the preprocessing function for observations\n",
    "ec.set_obs_transform(lambda x: preprocess_atari(x.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ejmejm/MLProjects/ludus/env.py\", line 127, in sim_thread\n",
      "    act = network.gen_act(obs)\n",
      "  File \"/home/ejmejm/MLProjects/ludus/policies.py\", line 62, in gen_act\n",
      "    return self._gen_discrete_act(obs)\n",
      "  File \"/home/ejmejm/MLProjects/ludus/policies.py\", line 50, in _gen_discrete_act\n",
      "    act_probs = self.sess.run(self.out_op, feed_dict={self.in_op: [obs]})\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1128, in _run\n",
      "    str(subfeed_t.get_shape())))\n",
      "ValueError: Cannot feed value of shape (1, 84, 84, 1) for Tensor 'input_1:0', which has shape '(?, 84, 84, 2)'\n",
      "\n",
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ejmejm/MLProjects/ludus/env.py\", line 127, in sim_thread\n",
      "    act = network.gen_act(obs)\n",
      "  File \"/home/ejmejm/MLProjects/ludus/policies.py\", line 62, in gen_act\n",
      "    return self._gen_discrete_act(obs)\n",
      "  File \"/home/ejmejm/MLProjects/ludus/policies.py\", line 50, in _gen_discrete_act\n",
      "    act_probs = self.sess.run(self.out_op, feed_dict={self.in_op: [obs]})\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1128, in _run\n",
      "    str(subfeed_t.get_shape())))\n",
      "ValueError: Cannot feed value of shape (1, 84, 84, 1) for Tensor 'input_1:0', which has shape '(?, 84, 84, 2)'\n",
      "\n",
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ejmejm/MLProjects/ludus/env.py\", line 127, in sim_thread\n",
      "    act = network.gen_act(obs)\n",
      "  File \"/home/ejmejm/MLProjects/ludus/policies.py\", line 62, in gen_act\n",
      "    return self._gen_discrete_act(obs)\n",
      "  File \"/home/ejmejm/MLProjects/ludus/policies.py\", line 50, in _gen_discrete_act\n",
      "    act_probs = self.sess.run(self.out_op, feed_dict={self.in_op: [obs]})\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1128, in _run\n",
      "    str(subfeed_t.get_shape())))\n",
      "ValueError: Cannot feed value of shape (1, 84, 84, 1) for Tensor 'input_1:0', which has shape '(?, 84, 84, 2)'\n",
      "\n",
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ejmejm/MLProjects/ludus/env.py\", line 127, in sim_thread\n",
      "    act = network.gen_act(obs)\n",
      "  File \"/home/ejmejm/MLProjects/ludus/policies.py\", line 62, in gen_act\n",
      "    return self._gen_discrete_act(obs)\n",
      "  File \"/home/ejmejm/MLProjects/ludus/policies.py\", line 50, in _gen_discrete_act\n",
      "    act_probs = self.sess.run(self.out_op, feed_dict={self.in_op: [obs]})\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/ejmejm/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1128, in _run\n",
      "    str(subfeed_t.get_shape())))\n",
      "ValueError: Cannot feed value of shape (1, 84, 84, 1) for Tensor 'input_1:0', which has shape '(?, 84, 84, 2)'\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-52235e329f0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Simualate env to generate data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mupdate_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_avg_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Append rewards to reward tracker list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Get all the data gathered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MLProjects/ludus/env.py\u001b[0m in \u001b[0;36mget_avg_reward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_avg_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_avg_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MLProjects/ludus/memory.py\u001b[0m in \u001b[0;36mget_avg_reward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrollout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mrollout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mall_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "update_rewards = []\n",
    "\n",
    "for i in range(int(n_episodes / batch_size)):\n",
    "    ec.sim_episodes(network, batch_size, max_steps) # Simualate env to generate data\n",
    "    update_rewards.append(ec.get_avg_reward()) # Append rewards to reward tracker list\n",
    "    dat = ec.get_data() # Get all the data gathered\n",
    "    \n",
    "#     processed_obs = np.concatenate([reshape_train_var(dat[:-1, 0]), reshape_train_var(dat[1:, 0])], axis=3)\n",
    "#     dat = dat[:-1]\n",
    "#     dat[:,0] = processed_obs\n",
    "    \n",
    "    network.train(dat) # Train the network with PPO\n",
    "    if i != 0 and i % print_freq == 0:\n",
    "        print(f'Update #{i}, Avg Reward: {np.mean(update_rewards[-print_freq:])}') # Print an update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec.render_episodes(network, 5, max_steps) # Render an episode to see the result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
